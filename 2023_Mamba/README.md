# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

## Abstract

基础模型现在为深度学习中的大多数令人兴奋的应用提供支持，几乎普遍基于 Transformer 架构及其核心注意模块。已经开发了许多次二次时间架构，例如线性注意力、门控卷积和循环模型和结构化状态空间模型 (SSM)，来解决 Transformer 在长序列上的计算效率低下的问题，但它们尚未执行以及对语言等重要模式的关注。我们发现此类模型的一个关键弱点是它们无法执行基于内容的推理，并进行了一些改进。首先，简单地让 SSM 参数作为输入的函数解决了它们在离散模态的弱点，允许模型根据当前标记沿序列长度维度选择性地传播或忘记信息。其次，即使这种变化阻止了高效卷积的使用，我们在循环模式下设计了一种硬件感知的并行算法。我们将这些选择性 SSM 集成到一个简化的端到端神经网络架构中，无需注意甚至 MLP 块 (Mamba)。Mamba 具有快速推理（比 Transformer 高 5 倍）和序列长度的线性缩放，其性能在高达百万长序列的真实数据上有所提高。作为通用序列模型主干，Mamba 在语言、音频和基因组学等多种模式上实现了最先进的性能。在语言建模方面，我们的 Mamba-3B 模型在预训练和下游评估中都优于相同大小的 Transformer，并匹配 Transformer 的大小的两倍。