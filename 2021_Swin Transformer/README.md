# VMamba: Visual State Space Model

## 针对任务

视觉表示学习

## 动机

CNN虽然可扩展性高，但是性能不如ViT，然而ViT存在二次计算复杂度的问题。作者从最近引入的状态空间模型中获得灵感，提出了视觉状态空间模型 (VMamba)，该模型在不牺牲全局感受野的情况下实现了线性复杂度。

Vamba 在有效降低注意力复杂性方面的成功背后的关键概念继承自选择性扫描空间状态序列模型 (S6)，最初设计用于解决自然语言处理 (NLP) 任务。与传统的注意力计算方法相比，S6 使 1-D 阵列（例如文本序列）中的每个元素能够通过压缩的隐藏状态与任何先前扫描的样本交互，有效地将二次复杂度降低到线性。

然而，由于视觉数据的非因果性质，直接将这种策略应用于修补和扁平的图像不可避免地会导致感受野受限，因为无法估计与未扫描补丁的关系。我们将此问题称为“方向敏感”问题，并提出通过新引入的 Cross-Scan 模块 (CSM) 对其进行解决。CSM 不是以单向模式（按列或逐行）遍历图像特征图的空间域，而是采用四向扫描策略，即从特征图中的所有四个角到相反的位置。该策略确保特征图中的每个元素都集成了来自不同方向所有其他位置的信息，在不增加线性计算复杂度的情况下呈现全局感受野。

VMamba 模型在 ImageNet-1K、COCO 和 ADE20K数据集上表现出有竞争力的性能。

![image-20240226141512500](https://cdn.jsdelivr.net/gh/ZL85/ImageBed@main//202402261415806.png)

信息流比较：注意与交叉扫描模块(CSM)。(a) 注意力机制统一集成中心像素的所有像素，导致$O(N^2)$复杂度。(b) CSM 集成了左上角、右下角、右上角和左下角的像素，$O(N)$复杂度。