# Revisiting Prototypical Network for Cross Domain Few-Shot Learning

重新审视用于跨域少样本学习的原型网络

## Abstract

原型网络（Prototypical Network）是一种流行的少样本学习解决方案，旨在利用深度神经网络建立一个对新颖的少样本分类（Few-Shot Classification, FSC）任务具有泛化能力的特征度量。然而，当将其推广到新领域的少样本分类任务时，其性能急剧下降。在本研究中，我们重新审视了这个问题，并认为问题在于神经网络中的简单偏差陷阱。具体来说，网络倾向于专注于一些带有偏见的捷径特征（例如颜色、形状等），这些特征在预定义域内的元训练任务中足以区分极少数类别，但无法在跨域上推广为一些理想的语义特征。为了缓解这个问题，我们提出了一种局部-全局蒸馏原型网络（LDP-net）。与标准的原型网络不同，我们建立了一个两分支网络，分别用于对查询图像和其随机局部裁剪进行分类。然后，在这两个分支之间进行知识蒸馏，以强化它们的类别关联一致性。其背后的理念是，由于这种全局-局部的语义关系预计在不同数据领域中都是成立的，因此局部-全局的蒸馏有利于利用一些跨域可转移的语义特征来建立特征度量。此外，这种局部-全局的语义一致性还在同一类别的不同图像之间加以强化，以减少结果特征的类内语义变化。此外，我们提出在训练过程中将局部分支更新为指数移动平均（EMA），这使得更好地提取跨训练周期的知识成为可能，并进一步增强了泛化性能。对八个跨领域的少样本分类基准进行的实验从经验上证明了我们的论点，并展示了 LDP-net 的 SOTA 成果。代码可在 https://github.com/NWPUZhoufei/LDP-Net 找到。

## Introduction

原型网络（ProtoNet）[1] 是一种流行的少样本分类（FSC）方法，其工作原理是利用深度神经网络建立一个对新颖的少样本任务具有泛化能力的特征度量。它采用基于情节的学习策略，其中每个情节（episode），例如 N-way K-shot，被构建为一个对比学习任务，以从少数支持样本所代表的有限类别集合中为每个查询样本确定正确的类别，这些类别由从少量支持样本中导出的原型表示。由于其框架的简单性和引人注目的少样本学习性能，ProtoNet 获得了广泛的研究关注 [2–5]。

然而，典型的ProtoNet在推广到新领域的少样本分类（FSC）任务时，例如将在自然图像上训练的ProtoNet应用于CUB中的细粒度鸟类图像，其性能显著下降。这严重限制了ProtoNet在实际应用中的可行性。在这项工作中，我们提出重新审视ProtoNet跨域泛化能力有限的内在原因，并在跨域设置中采用正确的方法使其恢复。具体而言，跨域泛化的关键，特别是在ProtoNet的少样本设置中，在于利用每个类别的一些语义信息，这些信息在不同领域中是不变的。为此，典型的ProtoNet利用了深度神经网络在特征学习中的强大表达能力。显然，它未能利用理想的跨域可转移语义特征。在这种情况下，深度神经网络获得了什么特征表示？一些最近的研究可能已经找到了可能的答案，即简单性偏差。研究表明，神经网络会专门抓住最简单的特征（例如颜色、形状等），并倾向于忽略复杂的预测特征（例如对象的语义）。受此启发，我们认为ProtoNet的跨域泛化能力有限是由于简单性偏差造成的，即它倾向于利用一些有偏见的捷径特征，这些特征在预定义域内的元训练任务中足以区分极少数类别，但在不同领域中很可能变化。

为了缓解这个问题，我们提出了一种局部-全局蒸馏原型网络（LDP-net），以识别图像特征和度量，从而更好地推广到新领域的少样本分类（FSC）任务。该网络采用了一个双分支结构。一个全局分支预测每个查询图像的类别关联，类似于标准的ProtoNet。一个局部分支处理从查询图像随机裁剪的图像块，并对这些局部裁剪进行分类预测。然后，我们在这两个分支之间进行知识蒸馏，以强制全局图像及其局部块具有一致的类别关联预测。其背后的原理是双重的。首先，与有偏见的视觉模式相比，全局图像和局部块之间的语义关系可以更普遍地保持，而不受数据域的影响。其次，局部-全局蒸馏可以将更丰富的语义信息从局部特征嵌入到最终的全局特征表示中，这已被证明更具有域不变性[11]。更进一步，我们将这种关联一致性约束应用于同一类别的图像之间。通过这样做，我们可以减少类内语义变化，并进一步提高图像特征表示的鲁棒性。此外，局部分支被更新为全局分支的指数移动平均（EMA），以产生稳健的分类预测，这使得我们的模型能够提炼跨训练周期的知识并增强泛化性能。一旦模型训练完成，仅保留全局分支作为特征提取器用于跨域FSC评估。值得注意的是，通过在新领域中简单地冻结特征提取器，所提出的方法在八个跨域FSC基准数据集上实现了最先进的结果。

本研究的主要贡献可总结如下：

- 我们从简单性偏差的角度审视了典型ProtoNet的有限跨域泛化能力，并提出了一种局部-全局知识蒸馏框架，有效缓解了这个问题。
- 所提出的LDP-Net具有深刻而创新的设计，能够学习到一个更好地推广到新领域的FSC任务的鲁棒特征度量。
- 所提出的LDP-Net在一系列跨域FSC基准测试中实现了最先进的性能。

## Related Work

**Few-shot learning：**Few-shot learning（FSL）旨在将在一些辅助基类中学到的知识推广到具有极少标记样本的新类别。流行的工作主要通过基于原型的度量学习 [1–3, 6]、元学习 [12–16] 和迁移学习 [17–19] 来解决FSL问题。基于原型的度量学习方法，例如ProtoNet [1]、MatchingNet [6] 等，侧重于学习一个嵌入空间，将同一类别的样本聚集在一起，并将不同类别的样本分开。在基于元学习的方法中，例如MAML [12]、MetaOptNet [13] 等，侧重于通过两阶段优化实现快速适应。LEO [14] 和HT [15] 利用超网络 [20] 生成任务感知参数，动态处理每个少样本任务。基于迁移学习的方法，例如S2M2 [18] 和Neg-Cosine [21]，侧重于学习良好的特征初始化，然后执行任务级微调以提高性能。

**Cross-domain few-shot learning：**跨域少样本学习（CD-FSL）与FSL不同，它侧重于在源域上学习一个模型，该模型能够有效地推广到目标域。根据所使用的训练数据，CD-FSL可以分为三种类型，例如，仅使用单个源域进行训练，使用多个源域进行训练，以及同时使用源域和目标域数据进行训练。其中，单源域CD-FSL更具挑战性和实用性，因此我们在这项工作中重点关注它。一些最近的工作在单源域CD-FSL上取得了进展。Doersch等人 [4] 基于交叉注意力为每个查询图像定制了一个具有空间感知的原型，并将自监督学习统一到元学习框架中，有效缓解了域偏移。由于使用了复杂的Transformer [28]，这种方法需要使用大规模的源域数据进行训练。Li等人 [5] 通过在特征空间中校准支持样本和查询样本之间的相对距离来实现了最先进的性能。Das等人 [25] 利用特征掩蔽器来过滤适用于目标域少样本任务的特征。Tseng等人 [29] 对特征采用任务特定的仿射变换来实现域适应。Wang等人 [22] 在输入样本上执行梯度更新以提高对域变化的鲁棒性。Guo等人 [23] 利用预训练结合微调来实现良好的性能，甚至比复杂的元学习算法更好。Liang等人 [24] 设计了一种基于特征重构的损失来微调每个少样本任务，并取得了显著的性能提升。尽管这些工作 [5, 22–25] 取得了进展，但在处理每个目标域上的少样本任务时，它们需要微调模型（即特征提取器）来缓解域差异。相比之下，所提出的方法侧重于学习具有强大泛化能力的模型，该模型能够在不经微调的情况下推广到广泛的目标域。

## Methodology

**Problem formulation**：在跨域少样本分类（CD-FSC）中，模型在源域数据集 Ds 上进行训练，然后在目标域数据集 Dt 上随机抽样一系列 N -way K-shot 的任务进行测试。需要注意的是，Ds 和 Dt 之间的类别没有重叠。在每个任务 T（即每个 episode）中，N 表示类别数量，K 表示每个类别的标记样本数量。N×K 个标记样本称为支持集 TS。此外，每个任务包含用于评估的查询集 TQ，其中包含与 TS 中相同类别的不同样本。通常，为了模拟少样本评估场景，模型训练也是以基于 episode 的方式进行的。

### Preliminary knowledge about the ProtoNet

ProtoNet 是一种流行的少样本学习器。它根据每个类别的支持样本构建原型，然后将查询样本与所有原型进行匹配。形式上，在给定一个少样本任务 T 的情况下，对应于每个类别的原型计算如下：

$\mathcal{C}_{n}=\frac{1}{K}\sum_{k=1}^{K}f_{\theta}\left(X_{S_{n,k}}\right),$

其中，fθ 表示特征提取器，Cn 表示类别 n 的原型，而 XSn,k 表示类别 n 的第 k 个支持样本。

然后，对于查询样本 XQi，分类预测 PQi 是通过与所有原型进行匹配得到的：

$\mathcal{P}_{Q_i}=\text{matching}\left(f_\theta(X_{Q_i}),\mathcal{C}_n\right),n\in\left[1,N\right],$

其中，matching(·) 表示两个特征之间的相似性匹配。对应于最大预测分数的标签被用作查询样本 XQi 的预测标签 ˆyQi。最后，交叉熵损失 H(·) 可以计算为：

$\mathcal{L}_{X_{Q_i}}^{ce}=H(\hat{y}_{Q_i},y_{Q_i}),$

其中，yQi 是查询图像 XQi 的真实标签。

### The proposed LDP-net

![image-20240325142912375](https://cdn.jsdelivr.net/gh/ZL85/ImageBed@main/202403251429603.png)

概述。如图1所示，所提出的LDP-net由一个双分支网络组成。其中，全局分支用于从输入图像中提取全局特征，其结构与标准ProtoNet中的特征提取器相同。局部分支将原始查询图像的随机裁剪作为输入，提取局部特征。在这两个分支之上，我们提出了一个局部-全局知识蒸馏模块，以强制在局部和全局特征之间建立一致性约束，该约束在不同域中被证明是不变的。此外，我们提出通过将局部分支更新为全局分支在元训练周期内的指数移动平均（EMA）来提炼跨训练周期的知识。

全局分支。在全局分支中，我们首先通过特征提取器 fθs 为每个图像提取全局特征。在本工作中，我们将由全局网络提取的原始图像对应的特征称为全局特征。然后，类似于ProtoNet，我们根据公式1自定义原型。最后，可以通过公式2计算查询图像 XQi 的分类预测 PQi。

局部分支。在局部分支中，我们为查询图像的局部裁剪提取局部特征，并利用之前定义的原型来计算每个局部特征的类别关联预测。一些最近的工作 [30–32] 表明，可以通过多裁剪增强有效地获取图像的局部信息。该增强首先随机裁剪原始图像，然后将其调整为较低分辨率以获取局部裁剪。因此，在本工作中，我们通过多裁剪增强来获取每个查询图像对应的局部裁剪。

具体而言，对于少样本任务 T 中的给定查询图像 XQi，我们首先通过多裁剪增强获得局部图像裁剪 Xqi,r，其中 r ∈ [1, R]，R 是裁剪数量。然后，我们通过局部网络 fθt 提取局部特征 fθt(Xqi,r)。类似地，我们根据原型计算每个局部特征 fθt(Xqi,r) 对应的分类预测 Pqi,r。

局部-全局知识蒸馏。我们鼓励全局图像表示从局部裁剪中提取更丰富的语义信息。通过这样做，我们可以强制局部和全局分支之间的语义一致性。为实现这一目标，我们对查询图像的全局和局部分类预测施加一致性约束。具体而言，对于给定的查询图像 XQi，我们分别利用全局分支和局部分支获得分类预测。然后，可以计算自图像局部-全局蒸馏损失如下：

$\mathcal{L}_{X_{Q_i}}^{self}=\frac{1}{R}\sum_{r=1}^{R}H\left(\mathcal{P}_{q_{i,r}},\mathcal{P}_{Q_i}\right),$

其中，PQi 和 Pqi,r 分别是给定查询图像 XQi 的全局分类预测和第 r 个局部分类预测。H(·) 表示交叉熵损失函数。

此外，为了减少类内语义变化，我们进一步强化了同一类别不同图像之间的局部-全局语义一致性。在实现中，我们发现使得同一类别所有查询样本的局部和全局预测保持一致会导致模型学习到无意义的解，从而导致模型崩溃。为了避免这个问题，对于查询图像 XQi，我们从查询集中的相同类别中随机选择一个查询图像 XQj，以强化跨图像的语义一致性。跨图像局部-全局蒸馏损失可以计算如下：

$\mathcal{L}_{X_{Q_i}}^{cross}=\dfrac{1}{R}\sum_{r=1}^{R}H\left(\mathcal{P}_{q_{j,r}},\mathcal{P}_{Q_i}\right),j\neq i,$

其中，Pqj,r 是查询图像 XQj 的局部分类预测。

在所提出的方法中，局部分支中的特征提取网络与全局分支中的特征提取网络具有相同的结构。一个简单的方法是根据损失函数同时更新两个分支网络。然而，这会引入额外的可学习参数，并且还会导致训练过程中的低效性。另一方面，基于 episode 的训练范式根据当前 episode 的梯度更新模型的参数。然而，元学习中的学习 episode 通常是从辅助数据集中随机抽样的，这意味着每个 episode 具有不同的类别组合。这与基于批次的训练形成鲜明对比，其中所有批次共享相同的类别集。独立解决这种具有显著语义空间变化的任务会迫使网络不断切换到不同的视觉模式组合，这是在学习 episode 间积累知识的一种低效方式。为了缓解这些问题，我们提出通过在元训练期间将局部分支更新为全局分支的指数移动平均值来提炼跨 episode 知识，从而更好地学习跨 episode 知识并进一步增强泛化性能。具体来说，我们将局部网络的参数更新为：

$\theta_t\leftarrow m\theta_t+(1-m)\theta_s,$

其中，θt 是局部分支 fθt 的参数，θs 是全局分支 fθs 的参数，而 m 是动量。

元训练。对于每个查询样本，我们还根据其全局预测计算交叉熵损失 Lce xqi 以促进原型学习。总之，对于一个少样本任务 T，所提出方法的总损失是：

$\mathcal{L}_{TS}=\sum_{i=1}^I\mathcal{L}_{X_{Q_i}}^{ce}+\lambda_1\cdot\sum_{i=1}^I\mathcal{L}_{X_{Q_i}}^{self}+\lambda_2\cdot\sum_{i=1}^I\mathcal{L}_{X_{Q_i}}^{cross},$

其中，I 表示任务 T 中的查询样本总数，λ1 和 λ2 是每个损失函数的权重系数。我们利用总损失来元训练全局分支。对于局部分支，我们根据公式6来更新它。元训练过程的详细流程总结在算法1中。一旦整个元训练完成，我们丢弃局部分支，将全局分支作为特征提取器用于跨域 FSC 评估。

![image-20240325143534207](https://cdn.jsdelivr.net/gh/ZL85/ImageBed@main/202403251435339.png)

在跨域 FSC 评估阶段，对于每个少样本任务，我们首先利用全局网络为每个图像提取特征。然后，我们使用支持集来训练 Logistic 回归分类器。最后，根据训练好的分类器对查询样本进行分类。值得注意的是，在目标域测试期间，所提出的方法不需要对特征提取器进行微调。

## Experimental Analysis

### Experimental details

数据集。在本工作中，我们专注于单源域 CD-FSL 问题。遵循标准基准数据集 [5, 22, 23]，我们将 mini-ImageNet [6] 数据集中包含的 64 类作为元训练集，作为源域进行训练。然后，我们在八个目标域数据集上验证泛化性能，即 CUB、Cars、Places、Plantae、ChestX、ISIC、EuroSAT 和 CropDisease。其中，CUB、Cars、Places 和 Plantae 是 [29] 中提出的，包含不同属性的自然图像。ChestX、ISIC、EuroSAT 和 CropDisease 是 [23] 中提出的跨域数据集，涵盖了医学、农业和遥感领域，存在显著的域偏移。所有图像都按照常见做法调整大小为 224×224 像素。

实现细节。如图1所示，所提出的方法包括一个全局分支和一个局部分支。对于全局分支，我们遵循 [5, 22, 23] 的做法，使用 ResNet10 作为特征提取网络，并通过在源域上进行传统基于批次的监督分类进行预训练。局部分支中的特征提取网络具有相同的结构。我们使用 Adam 优化器对网络进行 100 个周期的元训练，学习率设置为 0.001。在每个周期中，我们从源域中随机采样 100 个 episode。在每个 episode 中，除非另有说明，我们将类别数设置为 5，每个类别的支持样本数设置为 5，每个类别的查询样本数设置为 15。对于超参数，我们设置 λ1=1.0，λ2=0.15，m=0.998，R=6。由于我们没有用于模型选择的验证集，我们使用最后一个周期后的检查点作为最终模型。值得注意的是，所提出的方法只需要元训练模型一次，就可以直接部署到目标域，无需微调。

评估协议。我们按照标准的 CD-FSC 评估协议 [5, 23] 验证所提出的方法。在每个目标域中，我们随机抽样 600 个 N -way K-shot 15-query 任务，并计算这些抽样任务的平均准确率和 95% 的置信区间。在所有的验证实验中，我们设置 N=5 和 K=1 或 5。

### Experimental results

#### Comparison with the ProtoNet baseline

我们首先进行一些分析实验，将所提出的方法与 ProtoNet 进行比较。我们按照 [23] 中的实验设置，在 CD-FSC 基准测试中使用 ResNet-10 作为 ProtoNet 的骨干网络进行实现。此外，为了公平比较，我们以与所提出的方法相同的方式预训练 ProtoNet 的骨干网络。我们将预训练的 ProtoNet 标记为 ProtoNet++。值得注意的是，ProtoNet 使用欧几里得距离度量进行分类。因此，为了保持公平比较，在所有消融实验中，我们使用相同的距离度量进行分类。我们在两个自然图像跨域数据集（即 CUB 和 Cars）和两个极端跨域数据集（即 EuroSAT 和 ISIC）上进行实验。

定量比较。所提出方法与 ProtoNet 的比较结果如表1所示。可以看出，所提出的方法在所有数据集上的表现均优于 ProtoNet，提升范围为 3% 到 10%。与 ProtoNet++ 相比，所提出的方法在大多数情况下也实现了显著的性能提升。例如，在 CUB 数据集上，所提出的方法在1-shot和5-shot设置中分别比 ProtoNet++ 提高了7.36%和7.00%。在 EuroSAT 数据集上，虽然 ProtoNet++ 在 5-shot 设置中取得了更好的结果，但在1-shot 设置中，所提出的方法比 ProtoNet++ 提高了 4.6%。简而言之，与 ProtoNet 和 ProtoNet++ 相比，所提出的方法实现了显著的性能提升。这表明所提出的方法具有更好的跨域泛化能力。

定性分析。为了验证所提出的方法能够学习丰富的语义信息而不仅仅专注于最简单的特征，我们采用 CAM [37] 对特征进行可视化。可视化结果如图2所示。可以看出，ProtoNet++ 仅关注对象的一些局部区域，例如图2(b)，(e)，(h)，(k)。相比之下，所提出的方法可以关注对象的更广泛范围，例如图2(c)，(f)，(i)，(l)，这意味着所提出的方法可以捕捉更全面的语义信息，从而实现更好的泛化。

![image-20240325151010380](https://cdn.jsdelivr.net/gh/ZL85/ImageBed@main/202403251511043.png)

为了进一步说明所提出方法的泛化优势，我们可视化模型的损失景观。损失景观是李等人 [38] 提出的一种用于模型泛化验证的可视化工具。在实现中，我们在 2000 个不同的方向上随机扰动在源域训练的模型。然后，我们对每个扰动模型在目标域上进行推断，并记录损失值。最后，根据记录的损失值和方向可视化损失景观。在损失景观中，接近中心的轮廓描述了模型的最优解。轮廓越平滑，所对应的模型最优解所围成的空间越大，表明模型的泛化能力越好[38]。我们使用 CUB 数据集作为目标域来可视化模型的损失景观。所提出的 LDP-net 和 ProtoNet++ 之间的比较如图3所示。可以看出，与 ProtoNet++ 相比，所提出方法的最优解对应的轮廓更加平滑，所围成的空间更大。这表明所提出的方法具有更强的泛化能力。这一发现与定量实验结果相一致。总之，上述定量和定性实验表明，所提出的方法能够有效地减轻 ProtoNet 中的简单性偏见陷阱，学习可转移的语义知识，从而实现更好的跨域泛化。

![image-20240325151141443](https://cdn.jsdelivr.net/gh/ZL85/ImageBed@main/202403251511561.png)

#### Comparison with state-of-the-art methods

最先进的方法通常利用微调或利用少样本任务中的全部数据来提高性能。其中，微调指的是在目标域的每个少样本任务中更新在源域上训练的特征提取器。利用全部数据意味着在无监督的方式下也使用查询集中的样本。

我们根据是否需要微调以及是否使用全部数据将比较实验分为三种情况。情况1：既不需要微调也不使用全部数据，例如 RelationNet+ATA [22]、GNN+AFA [36] 等。情况2：不需要微调，但使用了全部数据，例如 TPN+ATA† [22] 和 RDC† [5]。情况3：既需要微调又需要使用全部数据，例如 TPN+ATA∗† [22] 和 RDC∗† [5]。为了与情况2中的方法进行公平比较，所提出的方法（LDP-net†）也利用了少样本任务中的全部数据。具体地，我们使用在支持集上训练的分类器对查询样本进行预测。然后，根据预测结果选择一些置信度高的查询样本作为支持集的扩充。最后，使用扩充后的支持集重新训练分类器。我们多次重复这个过程，并利用最后的分类器对查询样本进行测试，得到最终结果。



实验在八个目标域中分别进行了5-way 1-shot设置和5-way 5-shot设置的实验。对于每个设置，我们将八个目标域的平均结果作为总体评价。1-shot 和 5-shot 的结果分别显示在表2和表3中。对于情况1，所提出的方法（LDP-net）在大多数数据集上取得了最佳性能。总体而言，在 1-shot 设置中，所提出方法的平均结果达到了46.34%。与第二好的方法（GNN+AFA）相比，所提出的方法平均提升了1.49%。在 5-shot 设置中，所提出的方法的平均结果为62.60%，比第二好的方法（GNN+AFA）高出1.02%。对于情况2，在 1-shot 设置中，所提出的方法（LDP-net†）的平均结果达到了50.85%。与第二好的方法（RDC†）相比，所提出的方法实现了2.02%的提升。在 5-shot 设置中，所提出的方法在所有数据集上表现最佳。就平均结果而言，所提出的方法（LDP-net†）比第二好的对比方法（RDC†）提高了4.27%。对于情况3，尽管在目标域上冻结了特征提取器，但所提出的方法（LDP-net†）仍然在 1-shot 和 5-shot 设置下取得了最佳的平均结果。总之，所提出的方法在所有情况下都实现了最佳的平均性能。性能提升表明所提出的方法具有更强的跨域泛化能力。其背后的原因是所提出的方法能够在源域中学习更多知识，以促进在目标域上的泛化。所提出方法的另一个优点是特征提取器可以直接使用而无需微调，这显示了所提出方法的实用性。

![image-20240325150722111](C:/Users/ZL/AppData/Roaming/Typora/typora-user-images/image-20240325150722111.png)

![image-20240325150753289](C:/Users/ZL/AppData/Roaming/Typora/typora-user-images/image-20240325150753289.png)

#### Ablation study

所提出的方法主要包括三个组成部分，即自图像蒸馏（"Self-image"）、交叉图像蒸馏（"Cross-image"）和跨集数蒸馏（"Cross-episode"）。我们对每个组件进行了消融研究。值得注意的是，在所有的消融实验中，我们都使用欧几里得距离度量进行分类。消融结果如表 1 所示。首先，与ProtoNet++基线相比，当仅使用自图像蒸馏时，所提出的方法在大多数情况下表现更好。例如，在CUB数据集上，自图像蒸馏在1-shot和5-shot设置下分别比ProtoNet++提高了3.65%和1.86%。在加入交叉图像蒸馏后，所提出的方法在大多数情况下都可以观察到性能提升。此外，当添加了跨集数蒸馏后，性能进一步提升。特别是，在CUB数据集上，性能分别提高了3.66%和4.93%。总的来说，消融研究表明，所提出的方法的每个组件在跨域泛化中都发挥着重要作用，并且它们都对性能起到了积极的贡献。

![image-20240325150618540](C:/Users/ZL/AppData/Roaming/Typora/typora-user-images/image-20240325150618540.png)

## Conclusions

在本研究中，我们从简单性偏见的角度检查了标准ProtoNet在跨域泛化方面的不足，并提出了一种本地-全局知识蒸馏框架来缓解ProtoNet中的这一问题。通过同时强化来自同一图像的全局图像和局部裁剪以及同一类别其他图像之间的类别关联预测，我们的模型预期能够捕获更为鲁棒的语义信息，从而实现跨域泛化。此外，我们提出了一种跨集数知识蒸馏策略，进一步提高了所学特征和度量的泛化性能。所提出的方法在八个CD-FSC数据集上实现了最先进的结果。尽管对于CD-FSC任务取得了令人鼓舞的进展，但在推广到具有显著域漂移的领域（如胸部和ISIC）时，性能仍然远未令人满意。可能的解决方案包括增加训练数据的多样性以提取更多通用的元知识，或者提出更智能的模型适应策略，以更有效地将提取的知识集成到目标任务中。我们将把这些作为未来的工作。























