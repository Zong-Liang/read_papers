# StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning

StyleAdv：用于跨域小样本学习的原风格对抗训练

## Abstract

跨域小样本学习 (CD-FSL) 是最近一项新兴的任务，它解决了不同领域的小样本学习问题。它旨在将在源数据集中学到的先验知识转移到新的目标数据集。CD-FSL 任务尤其受到不同数据集的巨大领域差距的挑战。至关重要的是，这种领域差距实际上来自于视觉风格的变化，Wave-SAN 经验表明，跨越源数据的风格分布有助于缓解这个问题。然而，Wave-SAN 只是简单地交换两幅图像的风格。这样的普通操作使生成的样式“真实”和“简单”仍然属于源样式的原始集合。因此，受 vanilla 对抗性学习的启发，我们提出了一种用于 CD-FSL 的新型与模型无关的元风格对抗训练 (StyleAdv) 方法以及一种新颖的风格对抗性攻击方法。特别是，我们的风格攻击方法综合了“虚拟”和“硬”对抗性风格进行模型训练。这是通过用有符号的风格梯度扰动原始风格来实现的。通过不断攻击风格并迫使模型识别这些具有挑战性的对抗性风格，我们的模型逐渐对视觉风格具有鲁棒性，从而提高了新目标数据集的泛化能力。除了典型的基于 CNN 的主干之外，我们还在大规模预训练视觉转换器上使用我们的 StyleAdv 方法。在八个不同目标数据集上进行的大量实验证明了我们方法的有效性。无论是建立在 ResNet 还是 ViT 的基础上，我们实现了 CD-FSL 的最新技术。

## Introduction

本文研究了跨域少样本学习 (CD-FSL) 的任务，该任务旨在解决了不同领域的小样本学习 (FSL) 问题。作为 FSL 的一般方法，基于情节的元学习策略也被用于训练 CD-FSL 模型，例如，FWT、LRP、ATA 和 Wave-SAN。一般来说，为了在测试阶段模拟低样本状态，元学习采样情节来训练模型。每一个情节包含一个小的有标记的支持集和一个无标记的查询集。模型通过根据支持集预测查询集中包含的图像类别来学习元知识。学习到的元知识直接将模型泛化到新的目标类。

根据经验，我们发现源数据和目标数据之间视觉外观的变化是导致 CD-FSL 中域差异的关键原因之一。有趣的是，WaveSAN，我们之前的工作表明可以通过增强源图像的视觉风格来缓解域差异问题。特别是，Wave-SAN 建议以自适应实例归一化 (AdaIN) 的形式增加样式，方法是随机采样两个源情节并交换它们的风格。然而，尽管 Wave-SAN 很有效，但这种 näıve 风格的生成方法存在两个局限性：1) 交换操作使样式始终限制在源数据集的“真实”风格集中；2) 有限的真实风格进一步导致生成的风格过于“容易”而无法学习。因此，一个自然的问题是我们是否可以合成“虚拟”和“硬”风格来学习更强大的 CD-FSL 模型？形式上，我们使用“真实/虚拟”来指示风格是否最初出现在源风格集中，并将“简单/困难”定义为新风格是否使元任务更加困难。

为此，我们从对抗训练中汲取灵感，提出了一种新的 CD-FSL 元风格对抗训练方法 (StyleAdv)。StyleAdv 在元训练的两个迭代优化循环中玩极小极大游戏。特别是，内循环通过添加扰动从原始源风格生成对抗性风格。合成的对抗性风格对于当前的模型来说应该更具挑战性，从而增加了损失。虽然外循环通过最小化识别具有原始风格和对抗性风格的图像的损失来优化整个网络。我们的最终目标是使模型能够学习一个除了源数据相对有限和简单的风格之外，对各种风格具有鲁棒性的模型。这可以潜在地提高具有视觉外观变化的新目标域的泛化能力。

形式上，我们引入了一种新的风格对抗性攻击方法来支持 StyleAdv 的内部循环。与之前攻击方法不同，我们的风格攻击方法扰动和合成风格，而不是图像像素或特征。从技术上讲，我们首先从输入特征图中提取风格，并在前向计算链中包含提取的风格，以获得每个训练步骤的梯度。之后，我们通过将一定比例的梯度添加到原始风格来合成新的风格。我们的风格对抗性攻击方法合成的风格具有“硬”和“虚拟”的良好属性。特别是，由于我们以训练梯度的相反方向扰动风格，我们的生成会导致“硬”风格。我们的攻击方法产生了与原始源风格完全不同的完全“虚拟”风格。

关键的是，我们的风格攻击方法使用变化的风格扰动比使得风格合成是渐进式的，这使得它与普通对抗性攻击方法有很大不同。具体来说，我们提出了一种新颖的渐进式风格合成策略。直接插入扰动的 näıve 解决方案是单独攻击特征嵌入模块的每个块，然而，这可能会导致特征与高级块的偏差很大。因此，我们的策略是使当前块的合成信号由先前块的对抗性风格累积。另一方面，我们不是通过固定攻击率来攻击模型，而是通过从候选池中随机采样扰动率来合成新的风格。这有助于合成对抗风格的多样性。实验结果表明了我们方法的有效性：1) 我们的风格对抗性攻击方法确实合成了更具挑战性的风格，从而推动了源视觉分布的限制；2) 我们的 StyleAdv 显着提高了基础模型并优于所有其他 CD-FSL 竞争对手。

我们强调我们的 StyleAdv 与其他现有的 FSL 或 CD-FSL 模型无关和互补，例如 GNN 和 FWT。更重要的是，为了从大规模预训练模型 (例如 DINO)中受益，我们进一步探索了调整我们的 StyleAdv 以非参数方式改进 Vision Transformer (ViT)  主干。在实验中，我们表明 StyleAdv 不仅改进了基于 CNN 的 FSL/CD-FSL 方法，而且改进了大规模预训练的 ViT 模型。

最后，我们总结了我们的贡献。1) 针对 CD-FSL 提出了一种新的元风格对抗训练方法 StyleAdv。StyleAdv 通过先扰动原始风格，然后强制模型学习这种对抗性风格，提高了 CD-FSL 模型的鲁棒性。2) 我们提出了一种新的基于变化的攻击率的渐进合成策略的风格攻击方法，因此生成了不同的“虚拟”和“硬”风格。。3) 我们的方法与现有的 FSL 和 CD-FSL 方法是互补的；我们在基于 CNN 和基于 ViT 的主干上验证了我们的想法。4) 在 8 个未见的目标数据集的广泛结果表明，我们的 StyleAdv 优于以前的 CD-FSL 方法，构建了一个新的 SOTA 结果。

## Related Work

**Cross-Domain Few-Shot Learning：**旨在将模型从对海量标注的数据的依赖中解放出来的小样本学习已经研究了许多年。特别是，最近的一些工作，例如 CLIP、CoOp、CLIP-Adapter、Tip-Adapter 和 PMF 探索了用大规模预训练模型促进 FSL。特别是，PMF 贡献了一个简单的管道，并为 FSL 构建了一个 SOTA。作为FSL的扩展任务，CD-FSL 主要解决跨不同领域的 FSL。典型的基于元学习的 CD-FSL 方法包括 FWT、LRP、ATA、AFA 和 Wave-SAN。具体来说，FWT 和 LRP 通过细化批量归一化层并使用解释模型来指导训练来解决 CD-FSL。ATA、AFA 和 Wave-SAN 分别建议增强图像像素、特征和视觉风格。还探索了几种基于迁移学习的 CD-FSL 方法，例如 BSCD-FSL (也称为微调)、BSR 和 NSAE。这些方法表明微调有助于提高目标数据集的性能。引入额外数据或需要多个域数据集进行训练的其他工作包括 STARTUP、Meta-FDMixup、Me-D2N、TGDM、TriAE 和 DSL。

**Adversarial Attack：**对抗性攻击旨在通过向输入数据添加一些定制的扰动来误导模型。为了有效地产生扰动，人们提出了许多对抗性攻击方法。大多数工作攻击图像像素。具体来说，FGSM 和 PGD 是两个最经典和最著名的攻击算法。一些工作攻击特征空间。关键的是，很少有工作攻击风格。与旨在误导模型的这些工作不同，我们扰动风格来解决 CD-FSL 的视觉偏移问题。

**Adversarial Few-Shot Learning：**已经进行了几次探索 FSL 对抗学习的尝试。其中，MDAT、AQ 和MetaAdv 首先攻击输入图像，然后利用攻击图像训练模型，提高对抗样本的防御能力。Shen等人攻击情节的特征，提高 FSL 模型的泛化能力。请注意，ATA 和 AFA，两种 CD-FSL 方法也采用了对抗性学习。但是，我们与他们有很大的不同。ATA 和 AFA 扰动图像像素或特征，而我们的目标是通过生成不同的硬风格来弥合视觉差距。

**Style Augmentation for Domain Shift Problem：**在域生成、图像分割、人员重识别和 CD-FSL 中探索了增强风格分布以缩小域偏移问题。具体来说，MixStyle、AdvStyle、DSU 和 Wave-SAN 通过混合、攻击、从高斯分布采样和交换来合成没有额外参数的风格。MaxStyle 和L2D 需要额外的网络模块和复杂的辅助任务来帮助生成新的风格。通常，AdvStyle 是与我们最相关的工作。因此，我们强调关键差异：1) AdvStyle 攻击图像上的风格，而我们使用渐进式攻击方法在多个特征空间上攻击风格；2) AdvStyle 使用相同的任务损失 (分割) 进行攻击和优化；相比之下，我们使用经典的分类损失来攻击风格，同时利用任务损失 (FSL) 来优化整个网络。

## StyleAdv: Meta Style Adversarial Training

**Task Formulation：**情节 $\mathcal{T}=((S,Q),Y)$ 随机采样为每一个元任务的输入，其中 $Y$ 表示情节图像相对于的 $\mathcal{C}^{tr}$ 的全局类别标签。通常，每个元任务被构建为一个 $N$ 类 $K$ 标记问题，也就是说，对于每一集 $T$，将具有 $K$ 个标记图像的 $N$ 个类采样为支持集 $S$，并使用另一个 $M$ 个图像的相同 $N$ 个类构成查询集 $Q$。FSL 或 CD-FSL 模型根据 $S$ 预测 $Q$ 中的图像属于 $N$ 个类别的概率 $P$。形式上，我们有 $|S| = NK$ ，$|Q| = NM$ ， $|P| = NM×N$。

### Overview of Meta Style Adversarial Learning

为了减轻视觉外观变化导致的性能下降，我们通过促进模型对识别各种风格的鲁棒性来解决 CD-FSL。因此，我们将我们的 FSL 模型暴露于源数据集中存在的图像风格之外的一些具有挑战性的虚拟风格。为此，我们提出了一种新颖的 StyleAdv 对抗训练方法。关键的是，我们不是对图像像素添加扰动，而是特别关注对抗性地扰动风格。我们的 StyleAdv 的总体框架如图 1 所示。

![image-20240324153338137](https://cdn.jsdelivr.net/gh/ZL85/ImageBed@main/202403241643887.png)

我们的 StyleAdv 包含一个 CNN/ViT 主干 $E$，一个全局 FC 分类器 $f_{cls}$，和一个具有可学习参数$\theta_{E},\theta_{cls},\theta_{fsl}$ FSL 分类器 $f_{fsl}$。此外，我们还包括了我们的核心风格攻击方法、一种新颖的风格提取模块和 AdaIN。

总体而言，我们通过解决极小极大游戏来学习 StyleAdv。具体来说，极小极大游戏应该在每个元训练步骤中涉及两个迭代优化循环。特别是，

**内循环 (Inner loop)：**通过攻击原始源f风格来合成新的对抗性风格；生成的风格将增加当前网络的损失。

**外循环 (Outer loop)：**通过用原始风格和对抗性风格对源图像进行分类来优化整个网络；此过程将减少损失。

### Style Extraction from CNNs and ViTs

**Adaptive Instance Normalization (AdaIN)：**我们回顾了为 CNN 在风格迁移中提出的普通 AdaIN。特别是，AdaIN 表明实例级均值和标准差 (缩写为均值和标准差) 传达了输入图像的风格信息。将 $mean$ 和 $std$ 分别表示为 $\mu$ 和 $\sigma$，AdaIN (表示为 $A$) 表明 $F$ 的风格能通过将原始风格 $(\mu,\sigma)$ 替换为目标风格 $(\mu_{tgt},\sigma_{tgt})$ 迁移到 $F_{tgt}$ 的风格：

$\mathcal{A}(F,\mu_{tgt},\sigma_{tgt})=\sigma_{tgt}\frac{F-\mu(F)}{\sigma(F)}+\mu_{tgt}$

**Style Extraction for CNN Features：**如图 1 (b) 的上半部分所示，令 $F\in\mathcal{R}^{B\times C\times H\times W}$ 表示输入特征批次，其中 $B$，$C$，$H$，和 $W$ 分别表示特征 $F$ 的批量大小、通道、高度和宽度。与 AdaIN 一样，$F$ 的均值 $μ$ 和标准差 $\sigma$ 定义为：

$\mu(\mathrm{F})_{\mathrm{b,c}}=\frac{1}{HW}\sum_{h=1}^{H}\sum_{w=1}^{W}F_{b,c,h,w}$

$\sigma(\mathrm F)_{\mathrm b,\mathrm c}=\sqrt{\dfrac{1}{HW}\sum_{h=1}^{H}\sum_{w=1}^{W}(F_{b,c,h,w}-\mu_{\mathrm b,\mathrm c}(F))^2+\epsilon}$

其中 $\mu,\sigma\in\mathcal{R}^{B\times C}$。

**Meta Information Extraction for ViT Features：**我们探索了将 ViT 特征的元信息提取为 CNN 的方式。直观地说，这种元信息可以看作是 ViT 的唯一“风格”。如图 1 (b) 所示，我们以一个输入批次的图像数据分割为 $P\times P$ 大小的块为例。ViT 编码器将一个批次的块编码为类别 token 和 一个块 token。为了与 AdaIN 比较，我们将 $F_0$ 改变为 $F\in{\mathcal R}^{B\times C\times P\times P}$ 形状。此时，我们可以计算块 tokens $F$ 的元信息，如式5和式6所示。本质上，请注意 transformer 将位置嵌入集成到块表示中，因此可以认为空间关系仍然存在于块 tokens 中。这支持了我们将块 tokens $F_0$ 转换为空间特征图 $F$。在某种程度上，这可以通过通过大小为 $P\times P$ 的内核对输入数据应用卷积来实现（如图 1 (b) 中的虚线箭头所示）。

### Inner Loop: Style Adversarial Attack Method

我们提出了一种新的风格对抗性攻击方法——快速风格梯度符号方法（Style-FGSM）来完成内循环。如图 1 所示，给定一个输入源集 $(\mathcal{T},Y)$，我们首先将其送到主干 $E$ 和 FC 分类器 $f_{cls}$ 产生全局分类损失 $L_{cls}$（如图 ① 路径所示）。在这个过程中，关键步骤是使风格的梯度可用。为此，设 $F_{\mathcal{T}}$ 表示 $\mathcal{T}$ 的特征，我们得到 $F_{\mathcal{T}}$ 的风格 $(\mu,\sigma)$。之后，我们将原始集的特征转变为 $\mathcal{A}(F_{\mathcal{T}},\mu,\sigma)$。转换后的特征用于实际上地前向传播。通过这种方式，我们将 $\mu$ 和 $\sigma$ 包含在了前向计算链中，因此，我们可以访问它们的梯度。

利用 ② 路径的梯度，我们和 FGSM 做的一样，分别为 $\mu$ 和 $\sigma$ 通过添加一个小的有符号梯度比率来攻击 $\mu$ 和 $\sigma$ 。

$\mu^{adv}=\mu+\epsilon\cdot\operatorname{sign}(\nabla_{\mu}J(\theta_{E},\theta_{f_{cls}},\mathcal{A}(F_{\mathcal{T}},\mu,\sigma),Y))$

$\sigma^{adv}=\sigma+\epsilon\cdot\mathrm{sign}(\nabla_{\sigma}J(\theta_{E},\theta_{f_{cls}},\mathcal{A}(F_{\mathcal{T}},\mu,\sigma),Y))$

其中 $J()$ 是分类预测和真值之间的交叉熵损失，即 $\mathcal{L}_{cls}$。受 PGD 随机开始的启发，我们还将随机噪声 $k_{RT}\cdot\mathcal{N}(0,I)$ 在攻击前添加到 $(\mu,\sigma)$ 。$\mathcal{N}(0,I)$ 指高斯噪声，$k_{RT}$ 是一个超参数。我们的 Style-FGSM 使我们能够生成“虚拟”和“硬”风格。

**Progressive Style Synthesizing Strategy：**为了防止高级对抗特征偏离，我们建议在渐进式策略中应用我们的 Style-FGSM 。具体来说，嵌入模块 $E$ 有三个模块 $E_1$，$E_2$，$E_3$，对应特征$F_1$，$F_2$，$F_3$。对于第一个块，我们使用 $(\mu_{1},\sigma_{1})$ 来表示$F_1$ 的原始风格。对抗风格 $(\mu_1^{adv},\sigma_1^{adv})$ 直接通过公式 7 和公式 8 得到。对于后续块，当前块 $i$ 的攻击信号是从块 $1$ 累计到块 $i-1$ 的。以第二个块为例，块特征 $F_2$ 不是简单地由 $E_{2}(F_{1})$ 提取。相反，我们有 $F_{2}^{'}=E_{2}(F_{1}^{adv})$，其中 $F_{1}^{adv}=\mathcal{A}(F_{1},\mu_{1}^{adv},\sigma_{1}^{adv})$。在 $F_{2}^{'}$ 上的攻击产生对抗风格 $(\mu_{2}^{adv},\sigma_{2}^{adv})$。因此，我们为最后一个块生成了$(\mu_{3}^{adv},\sigma_{3}^{adv})$。渐进式攻击策略的说明附在附录中。

**Changing Style Perturbation Ratios：**与普通 FGSM 或 PGD 不同，我们的风格攻击算法有望合成具有多样性的新风格。因此，我们没有使用固定的攻击比率 $\epsilon$ ，而是从候选列表 $\epsilon_{list}$ 中随机抽取 $\epsilon$ 作为当前攻击比率。尽管 $\epsilon$ 的随机性，但我们仍然以更具挑战性的方向合成风格， $\epsilon$ 仅影响程度。